{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be60341-fe75-47c1-b33c-d7bacb1fe4ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will craft sophisticated ETL jobs that interface with a variety of common data sources, such as \n",
    "- REST APIs (HTTP endpoints)\n",
    "- RDBMS\n",
    "- Hive tables (managed tables)\n",
    "- Various file formats (csv, json, parquet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9fe8dc-6b2e-4499-8961-7e01309d05f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "d\n",
    "\n",
    "# Interview Questions\n",
    "\n",
    "As you progress through the practice, attempt to answer the following questions:\n",
    "\n",
    "## Columnar File\n",
    "- What is a columnar file format and what advantages does it offer?\n",
    "- Why is Parquet frequently used with Spark and how does it function?\n",
    "- How do you read/write data from/to a Parquet file using a DataFrame?\n",
    "\n",
    "## Partitions\n",
    "- How do you save data to a file system by partitions? (Hint: Provide the code)\n",
    "- How and why can partitions reduce query execution time? (Hint: Give an example)\n",
    "\n",
    "## JDBC and RDBMS\n",
    "- How do you load data from an RDBMS into Spark? (Hint: Discuss the steps and JDBC)\n",
    "\n",
    "## REST API and HTTP Requests\n",
    "- How can Spark be used to fetch data from a REST API? (Hint: Discuss making API requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7f0dcb-2214-41ae-a6f4-12d5a34506ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job One: Parquet file\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Data transformation requirements https://pgexercises.com/questions/aggregates/fachoursbymonth.html\n",
    "\n",
    "### Load\n",
    "Load data into a parquet file\n",
    "\n",
    "### What is Parquet? \n",
    "\n",
    "Columnar files are an important technique for optimizing Spark queries. Additionally, they are often tested in interviews.\n",
    "- https://www.youtube.com/watch?v=KLFadWdomyI\n",
    "- https://www.databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33324d02-bc67-4c31-b822-8fe8c69fead5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()\n",
    "\n",
    "members_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"members.csv\")\n",
    "bookings_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"bookings.csv\")\n",
    "facilities_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"facilities.csv\")\n",
    "\n",
    "members_df.createOrReplaceTempView(\"members\")\n",
    "bookings_df.createOrReplaceTempView(\"bookings\")\n",
    "facilities_df.createOrReplaceTempView(\"facilities\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        concat_ws(' ', mems.firstname, mems.surname) as member,\n",
    "        facs.name as facility\n",
    "    FROM\n",
    "        members mems\n",
    "        INNER JOIN bookings bks ON mems.memid = bks.memid\n",
    "        INNER JOIN facilities facs ON bks.facid = facs.facid\n",
    "    WHERE\n",
    "        facs.name IN ('Tennis Court 2', 'Tennis Court 1')\n",
    "    ORDER BY\n",
    "        member, facility\n",
    "\"\"\")\n",
    "\n",
    "result_df.write.mode(\"overwrite\").parquet(\"output.parquet\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51d425e-d532-47e5-8cbf-a91ca78246b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Two: Partitions\n",
    "\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Transform the data https://pgexercises.com/questions/joins/threejoin.html\n",
    "\n",
    "### Load\n",
    "Partition the result data by facility column and then save to `threejoin_delta` managed table. Additionally, they are often tested in interviews.\n",
    "\n",
    "hint: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html\n",
    "\n",
    "What are paritions? \n",
    "\n",
    "Partitions are an important technique to optimize Spark queries\n",
    "- https://www.youtube.com/watch?v=hvF7tY2-L3U&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aea2ca-5178-4034-91ee-c09942c5f518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()\n",
    "\n",
    "members_df = spark.read.format(\"delta\").load(\"/path/to/members_delta\")\n",
    "bookings_df = spark.read.format(\"delta\").load(\"/path/to/bookings_delta\")\n",
    "facilities_df = spark.read.format(\"delta\").load(\"/path/to/facilities_delta\")\n",
    "\n",
    "result_df = members_df.join(\n",
    "    bookings_df, \"memid\"\n",
    ").join(\n",
    "    facilities_df, \"facid\"\n",
    ").filter(\n",
    "    facilities_df.name.isin(\"Tennis Court 2\", \"Tennis Court 1\")\n",
    ").select(\n",
    "    concat_ws(\" \", members_df.firstname, members_df.surname).alias(\"member\"),\n",
    "    facilities_df.name.alias(\"facility\")\n",
    ").distinct().orderBy(\"member\", \"facility\")\n",
    "\n",
    "result_df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/threejoin_delta\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7610de14-acd6-4374-945d-661dbc08a08e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Three: HTTP Requests\n",
    "\n",
    "### Extract\n",
    "Extract daily stock price data price from the following companies, Google, Apple, Microsoft, and Tesla. \n",
    "\n",
    "Data Source\n",
    "- API: https://rapidapi.com/alphavantage/api/alpha-vantage\n",
    "- Endpoint: GET `TIME_SERIES_DAILY`\n",
    "\n",
    "Sample HTTP request\n",
    "\n",
    "```\n",
    "curl --request GET \\\n",
    "\t--url 'https://alpha-vantage.p.rapidapi.com/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=compact&datatype=json' \\\n",
    "\t--header 'X-RapidAPI-Host: alpha-vantage.p.rapidapi.com' \\\n",
    "\t--header 'X-RapidAPI-Key: [YOUR_KEY]'\n",
    "\n",
    "```\n",
    "\n",
    "Sample Python HTTP request\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"IBM\",\n",
    "    \"datatype\":\"json\",\n",
    "    \"outputsize\":\"compact\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Now 'data' contains the daily time series data for \"IBM\"\n",
    "```\n",
    "\n",
    "### Transform\n",
    "Find **weekly** max closing price for each company.\n",
    "\n",
    "hints: \n",
    "  - Use a `for-loop` to get stock data for each company\n",
    "  - Use the spark `union` operation to concat all data into one DF\n",
    "  - create a new `week` column from the data column\n",
    "  - use `group by` to calcualte max closing price\n",
    "\n",
    "### Load\n",
    "- Partition `DF` by company\n",
    "- Load the DF in to a managed table called, `max_closing_price_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b76fcc5-fc12-4401-a16c-e24c4c890dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, max\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StockPriceETL\").getOrCreate()\n",
    "\n",
    "companies = [\n",
    "    {\"name\": \"Google\", \"symbol\": \"GOOGL\"},\n",
    "    {\"name\": \"Apple\", \"symbol\": \"AAPL\"},\n",
    "    {\"name\": \"Microsoft\", \"symbol\": \"MSFT\"},\n",
    "    {\"name\": \"Tesla\", \"symbol\": \"TSLA\"}\n",
    "]\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "stock_data_df = spark.createDataFrame([], [\"date\", \"company\", \"symbol\", \"closing_price\"])\n",
    "\n",
    "for company_info in companies:\n",
    "    querystring = {\n",
    "        \"function\": \"TIME_SERIES_DAILY\",\n",
    "        \"symbol\": company_info[\"symbol\"],\n",
    "        \"datatype\": \"json\",\n",
    "        \"outputsize\": \"compact\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "    data = response.json()\n",
    "\n",
    "    daily_data = [(date, company_info[\"name\"], company_info[\"symbol\"], float(daily_data[\"4. close\"]))\n",
    "                  for date, daily_data in data[\"Time Series (Daily)\"].items()]\n",
    "\n",
    "    daily_data_df = spark.createDataFrame(daily_data, [\"date\", \"company\", \"symbol\", \"closing_price\"])\n",
    "    stock_data_df = stock_data_df.union(daily_data_df)\n",
    "\n",
    "stock_data_df = stock_data_df.withColumn(\"date\", to_date(stock_data_df[\"date\"], \"yyyy-MM-dd\"))\n",
    "stock_data_df = stock_data_df.withColumn(\"week\", F.weekofyear(stock_data_df[\"date\"]))\n",
    "\n",
    "window_spec = Window.partitionBy(\"company\", \"week\")\n",
    "max_closing_price_df = stock_data_df.withColumn(\"max_closing_price\", max(\"closing_price\").over(window_spec))\n",
    "\n",
    "max_closing_price_df = max_closing_price_df.drop(\"date\", \"closing_price\", \"week\", \"symbol\")\n",
    "max_closing_price_df = max_closing_price_df.dropDuplicates()\n",
    "\n",
    "max_closing_price_df.write.mode(\"overwrite\").saveAsTable(\"max_closing_price_weekly\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f98592-1f5f-4b42-9350-6720e69a7c22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ETL Job Four: RDBMS\n",
    "\n",
    "\n",
    "### Extract\n",
    "Extract RNA data from a public PostgreSQL database.\n",
    "\n",
    "- https://rnacentral.org/help/public-database\n",
    "- Extract 100 RNA records from the `rna` table (hint: use `limit` in your sql)\n",
    "- hint: use `spark.read.jdbc` https://docs.databricks.com/external-data/jdbc.html\n",
    "\n",
    "### Transform\n",
    "We want to load the data as it so there is no transformation required.\n",
    "\n",
    "\n",
    "### Load\n",
    "Load the DF in to a managed table called, `rna_100_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3011d775-d108-4cb0-85d1-bf21ae1c23d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RNADataETL\").getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://your_postgresql_host:5432/your_database\"\n",
    "properties = {\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "sql_query = \"SELECT * FROM rna LIMIT 100\"\n",
    "\n",
    "rna_df = spark.read.jdbc(url=jdbc_url, table=sql_query, properties=properties)\n",
    "\n",
    "rna_df.write.mode(\"overwrite\").saveAsTable(\"rna_100_records\")\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Spark ETL Jobs Exercieses",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
