{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d360d9e-9b3e-4375-9aef-3b2c3f91f1e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "bookings_df = spark.read.csv(\"path_to_cd_bookings.csv\", header=True, inferSchema=True)\n",
    "members_df = spark.read.csv(\"path_to_cd_members.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    members_df\n",
    "    .join(bookings_df, members_df.memid == bookings_df.memid)\n",
    "    .filter((members_df.firstname == 'David') & (members_df.surname == 'Farrell'))\n",
    "    .select(bookings_df.starttime)\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947bb779-fc49-4ee4-bd7c-8658d0cbd46a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "facilities_df = spark.read.csv(\"path_to_cd_facilities.csv\", header=True, inferSchema=True)\n",
    "bookings_df = spark.read.csv(\"path_to_cd_bookings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    facilities_df\n",
    "    .join(bookings_df, facilities_df.facid == bookings_df.facid)\n",
    "    .filter(\n",
    "        (col(\"name\").isin(['Tennis Court 2', 'Tennis Court 1'])) &\n",
    "        (col(\"starttime\") >= '2012-09-21') &\n",
    "        (col(\"starttime\") < '2012-09-22')\n",
    "    )\n",
    "    .select(col(\"starttime\").alias(\"start\"), col(\"name\"))\n",
    "    .orderBy(col(\"starttime\"))\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97843680-ae4a-4011-b62c-0eb885463e3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "members_df = spark.read.csv(\"path_to_cd_members.csv\", header=True, inferSchema=True)\n",
    "\n",
    "mems_df = members_df.alias(\"mems\")\n",
    "recs_df = members_df.alias(\"recs\")\n",
    "\n",
    "result_df = (\n",
    "    mems_df\n",
    "    .join(recs_df, col(\"recs.memid\") == col(\"mems.recommendedby\"), \"left_outer\")\n",
    "    .select(\n",
    "        col(\"mems.firstname\").alias(\"memfname\"),\n",
    "        col(\"mems.surname\").alias(\"memsname\"),\n",
    "        col(\"recs.firstname\").alias(\"recfname\"),\n",
    "        col(\"recs.surname\").alias(\"recsname\")\n",
    "    )\n",
    "    .orderBy(col(\"memsname\"), col(\"memfname\"))\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9141f22-3229-4c26-80fe-e0ee5e5adf16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "members_df = spark.read.csv(\"path_to_cd_members.csv\", header=True, inferSchema=True)\n",
    "bookings_df = spark.read.csv(\"path_to_cd_bookings.csv\", header=True, inferSchema=True)\n",
    "facilities_df = spark.read.csv(\"path_to_cd_facilities.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    members_df\n",
    "    .join(bookings_df, members_df.memid == bookings_df.memid)\n",
    "    .join(facilities_df, bookings_df.facid == facilities_df.facid)\n",
    "    .filter(facilities_df.name.isin(['Tennis Court 2', 'Tennis Court 1']))\n",
    "    .select(\n",
    "        concat_ws(' ', col(\"mems.firstname\"), col(\"mems.surname\")).alias(\"member\"),\n",
    "        col(\"facs.name\").alias(\"facility\")\n",
    "    )\n",
    "    .distinct()\n",
    "    .orderBy(\"member\", \"facility\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10cf6c0c-8ea1-4900-ab5c-ca313f23261c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "members_df = spark.read.csv(\"path_to_cd_members.csv\", header=True, inferSchema=True)\n",
    "\n",
    "mems_df = members_df.alias(\"mems\")\n",
    "recs_df = members_df.alias(\"recs\")\n",
    "\n",
    "result_df = (\n",
    "    mems_df\n",
    "    .select(\n",
    "        col(\"mems.firstname\").alias(\"member\"),\n",
    "        (\n",
    "            recs_df\n",
    "            .filter(col(\"recs.memid\") == col(\"mems.recommendedby\"))\n",
    "            .select(col(\"recs.firstname\"), col(\"recs.surname\"))\n",
    "            .withColumn(\"recommender\", col(\"recs.firstname\") + \" \" + col(\"recs.surname\"))\n",
    "            .limit(1)\n",
    "            .alias(\"recommender\")\n",
    "        )\n",
    "    )\n",
    "    .distinct()\n",
    "    .orderBy(\"member\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dba53b70-b71b-4ca5-b70f-810eba6334fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "members_df = spark.read.csv(\"path_to_cd_members.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    members_df\n",
    "    .filter(col(\"recommendedby\").isNotNull())\n",
    "    .groupBy(\"recommendedby\")\n",
    "    .count()\n",
    "    .orderBy(\"recommendedby\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "167cc072-df7d-4af5-9b3d-80374f53ad99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "bookings_df = spark.read.csv(\"path_to_cd_bookings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    bookings_df\n",
    "    .groupBy(\"facid\")\n",
    "    .agg(sum(\"slots\").alias(\"Total Slots\"))\n",
    "    .orderBy(\"facid\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cecb239-a4d9-4a1f-9ba4-93245b2d7a56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "bookings_df = spark.read.csv(\"path_to_cd_bookings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    bookings_df\n",
    "    .filter((col(\"starttime\") >= '2012-09-01') & (col(\"starttime\") < '2012-10-01'))\n",
    "    .groupBy(\"facid\")\n",
    "    .agg(sum(\"slots\").alias(\"Total Slots\"))\n",
    "    .orderBy(\"Total Slots\")\n",
    ")\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "220c09c3-f888-4b8f-8eef-1b44c9526934",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "bookings_df = spark.read.csv(\"path_to_cd_bookings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    bookings_df\n",
    "    .filter(year(col(\"starttime\")) == 2012)\n",
    "    .groupBy(\"facid\", month(col(\"starttime\")).alias(\"month\"))\n",
    "    .agg(sum(\"slots\").alias(\"Total Slots\"))\n",
    "    .orderBy(\"facid\", \"month\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80f1a654-5906-4377-afc0-3206bf75cc35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "bookings_df = spark.read.csv(\"path_to_cd_bookings.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    bookings_df\n",
    "    .select(countDistinct(\"memid\").alias(\"Count of Distinct memid\"))\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfdff632-d0b7-4d24-8e11-10c23168a4bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "bookings_df = spark.read.csv(\"path_to_cd_bookings.csv\", header=True, inferSchema=True)\n",
    "members_df = spark.read.csv(\"path_to_cd_members.csv\", header=True, inferSchema=True)\n",
    "\n",
    "bookings_df = bookings_df.withColumn(\"starttime\", col(\"starttime\").cast(\"timestamp\"))\n",
    "\n",
    "filtered_bookings_df = bookings_df.filter(col(\"starttime\") >= '2012-09-01')\n",
    "\n",
    "result_df = (\n",
    "    filtered_bookings_df\n",
    "    .join(members_df, on=[\"memid\"], how=\"inner\")\n",
    "    .groupBy(\"surname\", \"firstname\", \"memid\")\n",
    "    .agg(min(\"starttime\").alias(\"starttime\"))\n",
    "    .orderBy(\"memid\")\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9585c641-5ea6-4adc-ace8-bf506cccc873",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "members_df = spark.read.csv(\"path_to_cd_members.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    members_df\n",
    "    .select(concat_ws(', ', col(\"surname\"), col(\"firstname\")).alias(\"name\"))\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fddd613-dc6b-4e6b-934d-d3cce5591db2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "facilities_df = spark.read.csv(\"path_to_cd_facilities.csv\", header=True, inferSchema=True)\n",
    "\n",
    "result_df = (\n",
    "    facilities_df\n",
    "    .filter(col(\"name\").rlike(\"^TENNIS\", caseSensitive=False))\n",
    ")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90a727a9-e7d3-4374-ab0b-bb1dd1a665aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "# Load the 'cd.members' table as a DataFrame\n",
    "members_df = spark.read.csv(\"path_to_cd_members.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Perform the ETL operations\n",
    "result_df = (\n",
    "    members_df\n",
    "    .filter(col(\"telephone\").rlike('[()]'))\n",
    "    .select(\"memid\", \"telephone\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ac2c09c-e4bf-43aa-b67f-47fd67d499f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import substring, count\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "source_df = spark.read.csv(\"source_file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "source_df = source_df.withColumn(\"letter\", substring(source_df[\"surname\"], 1, 1))\n",
    "\n",
    "result_df = source_df.groupBy(\"letter\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "result_df = result_df.orderBy(\"letter\")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91306815-2684-4cd5-a806-e84e6da07229",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "sql_query = \"\"\"\n",
    "    SELECT DATE_ADD(to_date('2012-10-01'), seq) AS ts\n",
    "    FROM (SELECT posexplode(sequence(to_date('2012-10-01'), to_date('2012-10-31'), interval 1 day)) AS (seq, ts)) temp\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(sql_query)\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "911ba5b9-7d8b-4fc7-be40-1e2b88188d1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_trunc, count\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETLExample\").getOrCreate()\n",
    "\n",
    "source_df = spark.read.csv(\"source_file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "source_df = source_df.withColumn(\"month\", date_trunc(\"month\", source_df[\"starttime\"]))\n",
    "\n",
    "result_df = source_df.groupBy(\"month\").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "result_df = result_df.orderBy(\"month\")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2023-12-09 19:01:18",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
